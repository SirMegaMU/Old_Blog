---
layout: post
title: UA监测与伪装
date: 2021-08-30
tags: 爬虫
---

### UA监测

门户网站的服务器检测对应请求的身份标识，
若为浏览器，说明此请求为"正常"请求
若不是基于某一款浏览器，则为"不正常"请求，服务请就拒绝该请求

### UA伪装

通过修改请求头的User-Agent，把爬虫对应的身份标识伪装成浏览器，防止访问请求被拒绝。

**我们试着爬一下搜狗搜索返回的信息**

```python
import requests

if __name__ == '__main__':
    url1 = "https://www.sogou.com/web?"
    url2 = input("请输入要搜索的内容：")
    # url2 = "博客"
    param = {
        'query': url2
    }
    data_response = requests.get(url=url1, params=param)
    page_text = data_response.text
    file_name = url2 + '.html'
    with open(file_name, "w" , encoding = 'UTF-8') as file:
        file.write(page_text)
```

### 不进行UA伪装

我们先运行一下程序，打开我们保存的文件

**... ...**

**嗯**？

![img](https://sirmegamu.github.io/images/posts/2021-08-30/01.png)

### 进行UA伪装

```python
import requests

if __name__ == '__main__':
    url1 = "https://www.sogou.com/web?"
    url2 = input("请输入要搜索的内容：")
    # 加入伪装成火狐浏览器的User-Agent
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0"
    }
    param = {
        'query': url2
    }
    # 加入带有伪装的请求头
    data_response = requests.get(url=url1, params=param, headers=headers)
    page_text = data_response.text
    file_name = url2 + '.html'
    with open(file_name, "w", encoding="utf-8") as file:
        file.write(page_text)
```

现在我们再打开保存的html文件，看起来好多了

![img](https://sirmegamu.github.io/images/posts/2021-08-30/02.png)
